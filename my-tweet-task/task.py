"""Evaluators to optimize task: my-tweet-task.

THIS IS A TEMPLATE FOR YOU TO CHANGE!

Evaluators compute scores for prompts run over the configured dataset:
https://smith.langchain.com/o/e38c3ab3-b5c9-401b-bf29-fc663c4e80ac/datasets/5377af5b-9691-42c3-a4d9-47457b11dfc8
"""

from langchain_core.messages import AIMessage
from langsmith.schemas import Example, Run

# Modify these evaluators to measure the requested criteria.
# For most prompt optimization tasks, run.outputs["output"] will contain an AIMessage
# (Advanced usage) If you are defining a custom system to optimize, then the outputs will contain the object returned by your system


def example_evaluator(run: Run, example: Example) -> dict:
    """An example evaluator. Larger numbers are better."""
    # The Example object contains the inputs and reference labels from a single row in your dataset (if provided).

    # We've copied the inputs & outputs for the first example in the configured dataset.
    prompt_inputs = example.inputs
    # {
    #   "topic": "Gig Economy"
    # }
    reference_outputs = example.outputs  # aka labels
    # None - this example lacks expected outputs
    # The comments above autogenerated from example:
    # https://smith.langchain.com/o/e38c3ab3-b5c9-401b-bf29-fc663c4e80ac/datasets/5377af5b-9691-42c3-a4d9-47457b11dfc8/e/b7651935-f6a9-419d-a308-202356af784a

    # The Run object contains the full trace of your system. Typically you run checks on the outputs,
    # often comparing them to the reference_outputs
    predicted: AIMessage = run.outputs["output"]

    # Implement your evaluation logic here
    # score = len(str(predicted.content)) < 180  # Replace with actual score
    result = str(predicted.content)
    score = int("#" not in result)

    return {
        # The evaluator keys here define the metric you are measuring
        # You can provide further descriptions for these in the config.json
        "key": "tweet_omits_hashtags",
        "score": score,
        "comment": "Pass: tweet omits hashtags"
        if score == 1
        else "Fail: omit all hashtags from generated tweets",
    }


evaluators = [example_evaluator]
